{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "ME_D6xegjD81"
   },
   "outputs": [],
   "source": [
    "!pip install jsonlines python-dotenv lamini datasets langchain_groq"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "id": "qQog7K5zie8R"
   },
   "outputs": [],
   "source": [
    "import os\n",
    "import pandas as pd\n",
    "import pickle\n",
    "import jsonlines\n",
    "from pprint import pprint\n",
    "from dotenv import load_dotenv\n",
    "import lamini\n",
    "import logging\n",
    "from datasets import load_dataset, Dataset\n",
    "from transformers import AutoTokenizer, AutoModelForCausalLM\n",
    "from transformers import TrainingArguments, Trainer\n",
    "import torch\n",
    "from langchain_groq import ChatGroq\n",
    "from langchain_core.prompts import ChatPromptTemplate\n",
    "from langchain.evaluation.qa import QAEvalChain"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "zqsMbT9Fie8c"
   },
   "outputs": [],
   "source": [
    "os.environ['GROQ_API_KEY'] = 'GROQ_API_KEY'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "6hO-2CEcie9B"
   },
   "source": [
    "# Load dataset and tokenization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "7F_QMDbwie9C"
   },
   "outputs": [],
   "source": [
    "# load dataset from Hub\n",
    "finetuning_dataset = load_dataset(\"AmiraliSH/lamini\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "FawuveRzie9E",
    "outputId": "7711a740-855c-43ec-f0da-28ac690e1c01"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Dataset({\n",
      "    features: ['question', 'answer'],\n",
      "    num_rows: 1120\n",
      "})\n",
      "Dataset({\n",
      "    features: ['question', 'answer'],\n",
      "    num_rows: 280\n",
      "})\n"
     ]
    }
   ],
   "source": [
    "train_dataset, test_dataset = finetuning_dataset[\"train\"], finetuning_dataset[\"test\"]\n",
    "print(train_dataset)\n",
    "print(test_dataset)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "VzHfeiF6ie9E"
   },
   "outputs": [],
   "source": [
    "model_name = \"openai-community/gpt2\"\n",
    "tokenizer = AutoTokenizer.from_pretrained(model_name)\n",
    "model = AutoModelForCausalLM.from_pretrained(model_name)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "id": "EzFVKi6rOo2V"
   },
   "outputs": [],
   "source": [
    "# Use EOS token as padding if no pad token is set, ensuring compatibility during tokenization.\n",
    "if tokenizer.pad_token is None:\n",
    "    tokenizer.pad_token = tokenizer.eos_token"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "id": "DcicXW7gObC8"
   },
   "outputs": [],
   "source": [
    "def find_max_length(dataset, tokenizer):\n",
    "    \"\"\"\n",
    "    Finds the maximum tokenized length of concatenated question-answer pairs in the dataset.\n",
    "\n",
    "    Parameters\n",
    "    ----------\n",
    "    dataset : Dataset\n",
    "        The dataset containing \"question\" and \"answer\" fields.\n",
    "    tokenizer : transformers.PreTrainedTokenizer\n",
    "        The tokenizer used to convert text to token IDs.\n",
    "\n",
    "    Returns\n",
    "    -------\n",
    "    int\n",
    "        The maximum tokenized length found in the dataset.\n",
    "    \"\"\"\n",
    "    max_value = 0\n",
    "    for i in range(len(dataset)):\n",
    "        question, answer = dataset[\"question\"][i], dataset[\"answer\"][i]\n",
    "        tokenized_output_len = len(tokenizer(question+answer)[\"input_ids\"])\n",
    "        if tokenized_output_len > max_value:\n",
    "            max_value = tokenized_output_len\n",
    "    return max_value\n",
    "\n",
    "max_length = find_max_length(train_dataset, tokenizer)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "id": "RSOqLGSIie9F"
   },
   "outputs": [],
   "source": [
    "def inference(text, model, tokenizer, max_input_tokens=1000, max_output_tokens=100):\n",
    "    \"\"\"\n",
    "    Generates a model-based response for a given input text using a tokenizer and a language model.\n",
    "\n",
    "    Parameters\n",
    "    ----------\n",
    "    text : str\n",
    "        The input text to be processed and used as a prompt for the model.\n",
    "    model : transformers.PreTrainedModel\n",
    "        The pre-trained model used for generating text.\n",
    "    tokenizer : transformers.PreTrainedTokenizer\n",
    "        The tokenizer associated with the model, used for tokenizing the input text and decoding the output tokens.\n",
    "    max_input_tokens : int, optional\n",
    "        The maximum number of tokens for the input sequence. Default is 1000.\n",
    "    max_output_tokens : int, optional\n",
    "        The maximum number of tokens for the output sequence. Default is 100.\n",
    "\n",
    "    Returns\n",
    "    -------\n",
    "    str\n",
    "        The generated response text after removing the input prompt from the output.\n",
    "    \"\"\"\n",
    "\n",
    "    # Tokenize\n",
    "    inputs = tokenizer(\n",
    "            text,\n",
    "            return_tensors=\"pt\",\n",
    "            truncation=True,\n",
    "            max_length=max_input_tokens\n",
    "    )\n",
    "\n",
    "    input_ids = inputs[\"input_ids\"]\n",
    "    attention_mask = inputs[\"attention_mask\"]\n",
    "\n",
    "    # Generate\n",
    "    device = model.device\n",
    "    generated_tokens_with_prompt = model.generate(\n",
    "      input_ids=input_ids.to(device),\n",
    "      attention_mask=attention_mask.to(device),\n",
    "      max_new_tokens=max_output_tokens,\n",
    "      pad_token_id=tokenizer.eos_token_id\n",
    "    )\n",
    "\n",
    "    # Decode\n",
    "    generated_text_with_prompt = tokenizer.batch_decode(generated_tokens_with_prompt, skip_special_tokens=True)\n",
    "\n",
    "    # Strip the prompt\n",
    "    generated_text_answer = generated_text_with_prompt[0][len(text):]\n",
    "\n",
    "    return generated_text_answer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "I_XElYiyie9G",
    "outputId": "94d16045-622d-437d-d847-d4827d9a3eb2"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'answer': 'Lamini can be used for any type of content generation, including '\n",
      "           'creative writing. Try adapting one of our examples or walkthroughs '\n",
      "           'to your use case. You can find these examples in our '\n",
      "           'documentation.',\n",
      " 'question': '### Question:\\n'\n",
      "             'Are there any tutorials on using Lamini for content generation '\n",
      "             'in creative writing?\\n'\n",
      "             '\\n'\n",
      "             '### Answer:'}\n",
      "('\\n'\n",
      " '\\n'\n",
      " 'Lamini is a free, open source, open source, open source, open source, open '\n",
      " 'source, open source, open source, open source, open source, open source, '\n",
      " 'open source, open source, open source, open source, open source, open '\n",
      " 'source, open source, open source, open source, open source, open source, '\n",
      " 'open source, open source, open source, open source, open source, open '\n",
      " 'source, open source, open source, open source, open')\n"
     ]
    }
   ],
   "source": [
    "# Test the Interface for one sample\n",
    "test_sample = test_dataset[0]\n",
    "pprint(test_sample)\n",
    "pprint(inference(test_sample[\"question\"], model, tokenizer))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "id": "Vgs2xCOoie9G"
   },
   "outputs": [],
   "source": [
    "def tokenize_function(examples):\n",
    "    \"\"\"\n",
    "    Tokenizes question-answer pairs from the dataset with truncation.\n",
    "\n",
    "    Parameters\n",
    "    ----------\n",
    "    examples : dict\n",
    "        A batch of examples containing \"question\" and \"answer\" fields.\n",
    "\n",
    "    Returns\n",
    "    -------\n",
    "    dict\n",
    "        Tokenized inputs with the specified truncation and maximum length.\n",
    "    \"\"\"\n",
    "\n",
    "    text = [q+a for q, a in zip(examples[\"question\"], examples[\"answer\"])]\n",
    "\n",
    "    tokenizer.truncation_side = \"left\"\n",
    "    tokenized_output = tokenizer(\n",
    "        text,\n",
    "        return_tensors=\"np\",\n",
    "        truncation=True,\n",
    "        max_length=max_length\n",
    "    )\n",
    "\n",
    "    return tokenized_output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "qdKpLtXtie9G"
   },
   "outputs": [],
   "source": [
    "# Apply tokenizer on Train and Test datasets\n",
    "tokenized_train_dataset = train_dataset.map(\n",
    "    tokenize_function,\n",
    "    batched=True,\n",
    "    batch_size=32,\n",
    "    drop_last_batch=True\n",
    ")\n",
    "\n",
    "tokenized_test_dataset = test_dataset.map(\n",
    "    tokenize_function,\n",
    "    batched=True,\n",
    "    batch_size=32,\n",
    "    drop_last_batch=True\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "id": "8A8-_02Yie9J"
   },
   "outputs": [],
   "source": [
    "# While input_ids provides the input tokens, labels explicitly specifies the targets the model should learn to predict during training\n",
    "tokenized_train_dataset = tokenized_train_dataset.add_column(\"labels\", tokenized_train_dataset[\"input_ids\"])\n",
    "tokenized_test_dataset = tokenized_test_dataset.add_column(\"labels\", tokenized_test_dataset[\"input_ids\"])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "mcb_575rie9J"
   },
   "source": [
    "# Training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "mzhVqaAzie9K"
   },
   "outputs": [],
   "source": [
    "# Load base model\n",
    "base_model = AutoModelForCausalLM.from_pretrained(model_name)\n",
    "base_model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "fgyIgespie9K",
    "outputId": "3be60594-b27e-40b9-bc0a-bf017c43b69f"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "GPT2LMHeadModel(\n",
       "  (transformer): GPT2Model(\n",
       "    (wte): Embedding(50257, 768)\n",
       "    (wpe): Embedding(1024, 768)\n",
       "    (drop): Dropout(p=0.1, inplace=False)\n",
       "    (h): ModuleList(\n",
       "      (0-11): 12 x GPT2Block(\n",
       "        (ln_1): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "        (attn): GPT2SdpaAttention(\n",
       "          (c_attn): Conv1D()\n",
       "          (c_proj): Conv1D()\n",
       "          (attn_dropout): Dropout(p=0.1, inplace=False)\n",
       "          (resid_dropout): Dropout(p=0.1, inplace=False)\n",
       "        )\n",
       "        (ln_2): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "        (mlp): GPT2MLP(\n",
       "          (c_fc): Conv1D()\n",
       "          (c_proj): Conv1D()\n",
       "          (act): NewGELUActivation()\n",
       "          (dropout): Dropout(p=0.1, inplace=False)\n",
       "        )\n",
       "      )\n",
       "    )\n",
       "    (ln_f): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "  )\n",
       "  (lm_head): Linear(in_features=768, out_features=50257, bias=False)\n",
       ")"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Set the device\n",
    "if torch.cuda.is_available():\n",
    "    device = torch.device(\"cuda\")\n",
    "else:\n",
    "    device = torch.device(\"cpu\")\n",
    "\n",
    "base_model.to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "gzCt0_oCie9L",
    "outputId": "bc3db16f-6fe0-454c-d727-38c3f539854e"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Question:  ### Question:\n",
      "Are there any tutorials on using Lamini for content generation in creative writing?\n",
      "\n",
      "### Answer:\n",
      "----------------------------------------------------------------------------------------------------\n",
      "Correct answer:  Lamini can be used for any type of content generation, including creative writing. Try adapting one of our examples or walkthroughs to your use case. You can find these examples in our documentation.\n",
      "----------------------------------------------------------------------------------------------------\n",
      "Model's answer: \n",
      "\n",
      "The following examples are based on the original source code of the original game.\n",
      "\n",
      "Example 1: Creating a new game\n",
      "\n",
      "The first step is to create a new game.\n",
      "\n",
      "Create a new game.\n",
      "\n",
      "Create a new game.\n",
      "\n",
      "Create a new game.\n",
      "\n",
      "Create a new game.\n",
      "\n",
      "Create a new game.\n",
      "\n",
      "Create a new game.\n",
      "\n",
      "Create a new game.\n",
      "\n",
      "Create a new game.\n",
      "\n",
      "Create a new game\n"
     ]
    }
   ],
   "source": [
    "# Try base model before finetuning\n",
    "test_sample_question = test_dataset[\"question\"][0]\n",
    "test_sample_answer = test_dataset[\"answer\"][0]\n",
    "print(\"Question: \", test_sample_question); print(100*\"-\")\n",
    "print(\"Correct answer: \", test_sample_answer); print(100*\"-\")\n",
    "print(\"Model's answer:\", inference(test_sample_answer, base_model, tokenizer))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 35
    },
    "id": "QBe36a9Hie9M",
    "outputId": "e87dddc1-1435-421c-edd3-eebe8a4e5b21"
   },
   "outputs": [
    {
     "data": {
      "application/vnd.google.colaboratory.intrinsic+json": {
       "type": "string"
      },
      "text/plain": [
       "'lamini_docs_steps'"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Make a directory to save the model\n",
    "trained_model_name = f\"lamini_docs_steps\"\n",
    "output_dir = trained_model_name\n",
    "output_dir"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "id": "phEGrwY9ie9N"
   },
   "outputs": [],
   "source": [
    "training_args = TrainingArguments(\n",
    "\n",
    "  # Learning rate\n",
    "  learning_rate=1.0e-5,\n",
    "\n",
    "  # Number of training epochs\n",
    "  num_train_epochs=5,\n",
    "\n",
    "  # Max steps to train for (each step is a batch of data)\n",
    "  # Overrides num_train_epochs, if not -1\n",
    "  # max_steps=5,\n",
    "\n",
    "  # Batch size for training\n",
    "  per_device_train_batch_size=1,\n",
    "\n",
    "  # Directory to save model checkpoints\n",
    "  output_dir=output_dir,\n",
    "\n",
    "  # Other arguments\n",
    "  overwrite_output_dir=False, # Overwrite the content of the output directory\n",
    "  disable_tqdm=False, # Disable progress bars\n",
    "  eval_steps=120, # Number of update steps between two evaluations\n",
    "  save_steps=120, # After # steps model is saved\n",
    "  warmup_steps=1, # Number of warmup steps for learning rate scheduler\n",
    "  per_device_eval_batch_size=1, # Batch size for evaluation\n",
    "  eval_strategy=\"steps\",\n",
    "  logging_strategy=\"steps\",\n",
    "  logging_steps=1,\n",
    "  optim=\"adafactor\",\n",
    "  gradient_accumulation_steps = 4,\n",
    "  gradient_checkpointing=False,\n",
    "\n",
    "  # Parameters for early stopping\n",
    "  load_best_model_at_end=True,\n",
    "  save_total_limit=1,\n",
    "  metric_for_best_model=\"eval_loss\",\n",
    "  greater_is_better=False,\n",
    "\n",
    "  save_safetensors=False\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "T0AmwJtDie9N",
    "outputId": "ddac2138-fae9-4973-9537-78890cd830ec"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "GPT2LMHeadModel(\n",
      "  (transformer): GPT2Model(\n",
      "    (wte): Embedding(50257, 768)\n",
      "    (wpe): Embedding(1024, 768)\n",
      "    (drop): Dropout(p=0.1, inplace=False)\n",
      "    (h): ModuleList(\n",
      "      (0-11): 12 x GPT2Block(\n",
      "        (ln_1): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
      "        (attn): GPT2SdpaAttention(\n",
      "          (c_attn): Conv1D()\n",
      "          (c_proj): Conv1D()\n",
      "          (attn_dropout): Dropout(p=0.1, inplace=False)\n",
      "          (resid_dropout): Dropout(p=0.1, inplace=False)\n",
      "        )\n",
      "        (ln_2): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
      "        (mlp): GPT2MLP(\n",
      "          (c_fc): Conv1D()\n",
      "          (c_proj): Conv1D()\n",
      "          (act): NewGELUActivation()\n",
      "          (dropout): Dropout(p=0.1, inplace=False)\n",
      "        )\n",
      "      )\n",
      "    )\n",
      "    (ln_f): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
      "  )\n",
      "  (lm_head): Linear(in_features=768, out_features=50257, bias=False)\n",
      ")\n",
      "Memory footprint 0.510342192 GB\n",
      "Flops 973.721088 GFLOPs\n"
     ]
    }
   ],
   "source": [
    "# Calculate model FLOPs based on input shape and gradient accumulation steps.\n",
    "model_flops = (\n",
    "  base_model.floating_point_ops(\n",
    "    {\n",
    "       \"input_ids\": torch.zeros(\n",
    "           (1, max_length)\n",
    "      )\n",
    "    }\n",
    "  )\n",
    "  * training_args.gradient_accumulation_steps\n",
    ")\n",
    "\n",
    "print(base_model)\n",
    "print(\"Memory footprint\", base_model.get_memory_footprint() / 1e9, \"GB\")\n",
    "print(\"Flops\", model_flops / 1e9, \"GFLOPs\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {
    "id": "p2-tNZAvie9P"
   },
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "Initialize the Trainer to manage the entire training process, including model training, evaluation,\n",
    "and logging. It uses the provided model, training arguments, and tokenized datasets for training\n",
    "and evaluation, handling tasks like optimization, checkpointing, and metric calculation.\n",
    "\"\"\"\n",
    "trainer = Trainer(\n",
    "    model=base_model,\n",
    "    args=training_args,\n",
    "    train_dataset=tokenized_train_dataset,\n",
    "    eval_dataset=tokenized_test_dataset,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 652
    },
    "id": "ioHAixoKie9P",
    "outputId": "d3f08dd6-8674-4372-9d43-78705fc633af"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m The `run_name` is currently set to the same value as `TrainingArguments.output_dir`. If this was not intended, please specify a different run name by setting the `TrainingArguments.run_name` parameter.\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Using wandb-core as the SDK backend. Please refer to https://wandb.me/wandb-core for more information.\n"
     ]
    },
    {
     "data": {
      "application/javascript": "\n        window._wandbApiKey = new Promise((resolve, reject) => {\n            function loadScript(url) {\n            return new Promise(function(resolve, reject) {\n                let newScript = document.createElement(\"script\");\n                newScript.onerror = reject;\n                newScript.onload = resolve;\n                document.body.appendChild(newScript);\n                newScript.src = url;\n            });\n            }\n            loadScript(\"https://cdn.jsdelivr.net/npm/postmate/build/postmate.min.js\").then(() => {\n            const iframe = document.createElement('iframe')\n            iframe.style.cssText = \"width:0;height:0;border:none\"\n            document.body.appendChild(iframe)\n            const handshake = new Postmate({\n                container: iframe,\n                url: 'https://wandb.ai/authorize'\n            });\n            const timeout = setTimeout(() => reject(\"Couldn't auto authenticate\"), 5000)\n            handshake.then(function(child) {\n                child.on('authorize', data => {\n                    clearTimeout(timeout)\n                    resolve(data)\n                });\n            });\n            })\n        });\n    ",
      "text/plain": [
       "<IPython.core.display.Javascript object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[34m\u001b[1mwandb\u001b[0m: Logging into wandb.ai. (Learn how to deploy a W&B server locally: https://wandb.me/wandb-server)\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: You can find your API key in your browser here: https://wandb.ai/authorize\n",
      "wandb: Paste an API key from your profile and hit enter, or press ctrl+c to quit:"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " ··········\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[34m\u001b[1mwandb\u001b[0m: Appending key for api.wandb.ai to your netrc file: /root/.netrc\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "Tracking run with wandb version 0.18.5"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Run data is saved locally in <code>/content/wandb/run-20241112_101350-da5cowz2</code>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Syncing run <strong><a href='https://wandb.ai/a-sahraei98-/huggingface/runs/da5cowz2' target=\"_blank\">lamini_docs_steps</a></strong> to <a href='https://wandb.ai/a-sahraei98-/huggingface' target=\"_blank\">Weights & Biases</a> (<a href='https://wandb.me/run' target=\"_blank\">docs</a>)<br/>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View project at <a href='https://wandb.ai/a-sahraei98-/huggingface' target=\"_blank\">https://wandb.ai/a-sahraei98-/huggingface</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View run at <a href='https://wandb.ai/a-sahraei98-/huggingface/runs/da5cowz2' target=\"_blank\">https://wandb.ai/a-sahraei98-/huggingface/runs/da5cowz2</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='1400' max='1400' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [1400/1400 07:05, Epoch 5/5]\n",
       "    </div>\n",
       "    <table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       " <tr style=\"text-align: left;\">\n",
       "      <th>Step</th>\n",
       "      <th>Training Loss</th>\n",
       "      <th>Validation Loss</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>120</td>\n",
       "      <td>2.981300</td>\n",
       "      <td>2.223344</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>240</td>\n",
       "      <td>2.051900</td>\n",
       "      <td>2.037769</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>360</td>\n",
       "      <td>1.634700</td>\n",
       "      <td>1.937185</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>480</td>\n",
       "      <td>1.960900</td>\n",
       "      <td>1.876956</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>600</td>\n",
       "      <td>2.182900</td>\n",
       "      <td>1.841003</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>720</td>\n",
       "      <td>1.915600</td>\n",
       "      <td>1.819551</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>840</td>\n",
       "      <td>1.673900</td>\n",
       "      <td>1.797953</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>960</td>\n",
       "      <td>1.994100</td>\n",
       "      <td>1.787369</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1080</td>\n",
       "      <td>1.639100</td>\n",
       "      <td>1.774583</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1200</td>\n",
       "      <td>1.537600</td>\n",
       "      <td>1.769493</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1320</td>\n",
       "      <td>1.647500</td>\n",
       "      <td>1.765734</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table><p>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "training_output = trainer.train()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "UqDSRiMUie9Q",
    "outputId": "21f2794f-5ee0-4e33-bc57-7a43dae6cd26"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Saved model to: lamini_docs_steps/final\n"
     ]
    }
   ],
   "source": [
    "# Save the model\n",
    "save_dir = f'{output_dir}/final'\n",
    "\n",
    "trainer.save_model(save_dir)\n",
    "print(\"Saved model to:\", save_dir)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "7C2m46SHie9Q"
   },
   "source": [
    "# Evaluation with another LLM"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "dK37X_ygie9Q",
    "outputId": "972f0465-18d7-42c0-c962-7040228a4205"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "GPT2LMHeadModel(\n",
       "  (transformer): GPT2Model(\n",
       "    (wte): Embedding(50257, 768)\n",
       "    (wpe): Embedding(1024, 768)\n",
       "    (drop): Dropout(p=0.1, inplace=False)\n",
       "    (h): ModuleList(\n",
       "      (0-11): 12 x GPT2Block(\n",
       "        (ln_1): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "        (attn): GPT2SdpaAttention(\n",
       "          (c_attn): Conv1D()\n",
       "          (c_proj): Conv1D()\n",
       "          (attn_dropout): Dropout(p=0.1, inplace=False)\n",
       "          (resid_dropout): Dropout(p=0.1, inplace=False)\n",
       "        )\n",
       "        (ln_2): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "        (mlp): GPT2MLP(\n",
       "          (c_fc): Conv1D()\n",
       "          (c_proj): Conv1D()\n",
       "          (act): NewGELUActivation()\n",
       "          (dropout): Dropout(p=0.1, inplace=False)\n",
       "        )\n",
       "      )\n",
       "    )\n",
       "    (ln_f): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "  )\n",
       "  (lm_head): Linear(in_features=768, out_features=50257, bias=False)\n",
       ")"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Transfer the model to evaluation mode\n",
    "finetuned_slightly_model = AutoModelForCausalLM.from_pretrained(save_dir, local_files_only=True)\n",
    "finetuned_slightly_model.to(device)\n",
    "finetuned_slightly_model.eval()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {
    "id": "UO5ws0wrie9R"
   },
   "outputs": [],
   "source": [
    "# Prepare test data for evaluation\n",
    "eval_dataset = [{\"query\": q, \"answer\": a} for q, a in zip(test_dataset[\"question\"], test_dataset[\"answer\"])]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {
    "id": "2vP46JcKie9S"
   },
   "outputs": [],
   "source": [
    "# Prediction on evaluation dataset\n",
    "prediction = []\n",
    "for sample in eval_dataset:\n",
    "    question = sample[\"query\"]\n",
    "    answer = sample[\"answer\"]\n",
    "    finetuned_model_answer = inference(question, finetuned_slightly_model, tokenizer)\n",
    "    output = {\"query\": question, \"answer\": answer, \"result\": finetuned_model_answer}\n",
    "    prediction.append(output)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 35
    },
    "id": "ovAa60f2ie9S",
    "outputId": "623da6ff-2108-4a37-8272-7fb5c188b96e"
   },
   "outputs": [
    {
     "data": {
      "application/vnd.google.colaboratory.intrinsic+json": {
       "type": "string"
      },
      "text/plain": [
       "'The answer to the math problem 1+1 is 2.'"
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# LLM model used for evaluation\n",
    "llm = ChatGroq(model=\"llama3-groq-70b-8192-tool-use-preview\", temperature=0)\n",
    "prompt = \"solve this math problem: {math_problem}\"\n",
    "math_problem = \"1+1\"\n",
    "prompt_template = ChatPromptTemplate.from_template(prompt)\n",
    "customer_message = prompt_template.format_messages(math_problem=math_problem)\n",
    "llm.invoke(customer_message).content"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {
    "id": "VVLZM0cvie9T"
   },
   "outputs": [],
   "source": [
    "actual = [\n",
    "    {\"query\": \"Sentence 1\", \"answer\": \"The dog is thirsty\"},\n",
    "    {\"query\": \"Sentence 2\", \"answer\": \"The dog is hungry\"},\n",
    "]\n",
    "\n",
    "pred = [\n",
    "    {\"query\": \"Sentence 1\", \"answer\": \"The dog is thirsty\", \"result\": \"The dog is thirsty\"},\n",
    "    {\"query\": \"Sentence 2\", \"answer\": \"The dog is hungry\", \"result\": \"The dog is thirsty\"},\n",
    "]\n",
    "# result = eval_chain.evaluate(actual, pred)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {
    "id": "warvYojLie9T"
   },
   "outputs": [],
   "source": [
    "# Initialize evaluation chain and evaluate predictions\n",
    "eval_chain = QAEvalChain.from_llm(llm)\n",
    "result = eval_chain.evaluate(eval_dataset, prediction)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "-Pn6X0ONie9U",
    "outputId": "7b78fe45-9a6a-46b7-934d-b53aeb2cb68a"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy: 23.57%\n"
     ]
    }
   ],
   "source": [
    "# Count the accuracy measure\n",
    "eval_result = [1 if i[\"results\"] == \"GRADE: CORRECT\" else 0 for i in result]\n",
    "eval_result = (sum(eval_result) / len(eval_result)) * 100\n",
    "print(f\"Accuracy: {eval_result:.2f}%\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 779
    },
    "id": "LSyvfypp7zZT",
    "outputId": "ac4a23a3-af68-43ce-e7bf-e6b68584a710"
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<style type=\"text/css\">\n",
       "#T_91b73_row0_col0, #T_91b73_row0_col1, #T_91b73_row0_col2, #T_91b73_row1_col0, #T_91b73_row1_col1, #T_91b73_row1_col2, #T_91b73_row2_col0, #T_91b73_row2_col1, #T_91b73_row2_col2, #T_91b73_row3_col0, #T_91b73_row3_col1, #T_91b73_row3_col2, #T_91b73_row4_col0, #T_91b73_row4_col1, #T_91b73_row4_col2 {\n",
       "  text-align: left;\n",
       "  vertical-align: text-top;\n",
       "}\n",
       "</style>\n",
       "<table id=\"T_91b73\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr>\n",
       "      <th class=\"blank level0\" >&nbsp;</th>\n",
       "      <th id=\"T_91b73_level0_col0\" class=\"col_heading level0 col0\" >query</th>\n",
       "      <th id=\"T_91b73_level0_col1\" class=\"col_heading level0 col1\" >answer</th>\n",
       "      <th id=\"T_91b73_level0_col2\" class=\"col_heading level0 col2\" >result</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th id=\"T_91b73_level0_row0\" class=\"row_heading level0 row0\" >0</th>\n",
       "      <td id=\"T_91b73_row0_col0\" class=\"data row0 col0\" >### Question:\n",
       "Are there any tutorials on using Lamini for content generation in creative writing?\n",
       "\n",
       "### Answer:</td>\n",
       "      <td id=\"T_91b73_row0_col1\" class=\"data row0 col1\" >Lamini can be used for any type of content generation, including creative writing. Try adapting one of our examples or walkthroughs to your use case. You can find these examples in our documentation.</td>\n",
       "      <td id=\"T_91b73_row0_col2\" class=\"data row0 col2\" >Yes, there are tutorials available on using Lamini for content generation in creative writing. These include tutorials on using Lamini for content generation in creative writing, examples of using Lamini for content generation in creative writing, and examples of using Lamini for content generation in creative writing. Additionally, there are tutorials available on using Lamini for content generation in creative writing using examples of using Lamini for content generation in creative writing. Additionally, there are tutorials available on using Lamini for content generation in creative</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th id=\"T_91b73_level0_row1\" class=\"row_heading level0 row1\" >1</th>\n",
       "      <td id=\"T_91b73_row1_col0\" class=\"data row1 col0\" >### Question:\n",
       "Can Lamini be used to perform sentiment analysis or opinion mining on large volumes of text data?\n",
       "\n",
       "### Answer:</td>\n",
       "      <td id=\"T_91b73_row1_col1\" class=\"data row1 col1\" >Lamini can be used for sentiment analysis or opinion mining on large volumes of text data. To learn how, check out walkthroughs and examples available on Lamini’s website. With some imagination, you can adapt those examples to your data and use case.</td>\n",
       "      <td id=\"T_91b73_row1_col2\" class=\"data row1 col2\" >Yes, Lamini can be used to perform sentiment analysis or opinion mining on large volumes of text data. It can be used to generate text that is representative of a specific sentiment or sentiment index, or to generate text that is representative of a specific sentiment or sentiment index. It can also be used to generate text that is representative of a specific sentiment or sentiment index, or to generate text that is representative of a specific sentiment or sentiment index. Additionally, Lamini can be used to generate text that is</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th id=\"T_91b73_level0_row2\" class=\"row_heading level0 row2\" >2</th>\n",
       "      <td id=\"T_91b73_row2_col0\" class=\"data row2 col0\" >### Question:\n",
       "Do I have to pay for using Lamini?\n",
       "\n",
       "### Answer:</td>\n",
       "      <td id=\"T_91b73_row2_col1\" class=\"data row2 col1\" >Everyone starts with 10,000 free credits, which is equivalent to about $100. After that, you can purchase more credits in the “API” tab at app.lamini.ai.</td>\n",
       "      <td id=\"T_91b73_row2_col2\" class=\"data row2 col2\" >Yes, you can use Lamini for free to use your own data. You can use Lamini for free to use your own data. You can use Lamini for free to use your own data. You can use Lamini for free to use your own data. You can use Lamini for free to use your own data. You can use Lamini for free to use your own data. You can use Lamini for free to use your own data. You can use Lamini for free</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th id=\"T_91b73_level0_row3\" class=\"row_heading level0 row3\" >3</th>\n",
       "      <td id=\"T_91b73_row3_col0\" class=\"data row3 col0\" >### Question:\n",
       "Can Lamini understand and generate text in multiple languages?\n",
       "\n",
       "### Answer:</td>\n",
       "      <td id=\"T_91b73_row3_col1\" class=\"data row3 col1\" >Yes, Lamini can understand and generate text in multiple languages. It currently supports over 20 languages, including English, Spanish, French, German, Chinese, and Japanese.</td>\n",
       "      <td id=\"T_91b73_row3_col2\" class=\"data row3 col2\" >Yes, Lamini can understand and generate text in multiple languages. It can generate text in multiple languages by using the language model's built-in language model engine, which can be used to generate text in multiple languages. Additionally, Lamini can generate text in multiple languages by using the language model's built-in language model engine, which can be used to generate text in multiple languages. Additionally, Lamini can generate text in multiple languages by using the language model's built-in language model engine</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th id=\"T_91b73_level0_row4\" class=\"row_heading level0 row4\" >4</th>\n",
       "      <td id=\"T_91b73_row4_col0\" class=\"data row4 col0\" >### Question:\n",
       "Can Lamini talk to animals or understand what they're saying?\n",
       "\n",
       "### Answer:</td>\n",
       "      <td id=\"T_91b73_row4_col1\" class=\"data row4 col1\" >While Lamini possesses extraordinary linguistic capabilities, it is crucial to note that its abilities do not extend to conversing with our animal counterparts or comprehending their communications. As an AI language model, Lamini's domain of expertise revolves around processing and generating text, responding to human inquiries and prompts with remarkable precision. While the enigmatic language of animals remains beyond its purview, Lamini's prowess in linguistic understanding and contextual interpretation continues to astound, forging new frontiers in human-machine interactions. While our fascination with bridging the gap between human and animal communication endures, Lamini's current capacities remain focused on enhancing our understanding of language and facilitating meaningful dialogue in the realms of human discourse.</td>\n",
       "      <td id=\"T_91b73_row4_col2\" class=\"data row4 col2\" >Yes, Lamini can talk to animals or understand what they're saying. It can learn from their responses and use them to improve its language models. This can be useful for generating text that is more conversational or nuanced. Additionally, Lamini can learn from the animals' responses and use them to improve its language models. This can be useful for generating text that is more conversational or nuanced. Additionally, Lamini can learn from the animals' responses and use them to improve its language models.</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n"
      ],
      "text/plain": [
       "<pandas.io.formats.style.Styler at 0x7c3f92110400>"
      ]
     },
     "execution_count": 37,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Save the result of evaluation in a Datafraem\n",
    "eval_df = pd.DataFrame.from_dict(prediction)\n",
    "head_of_eval_df = eval_df.head()\n",
    "style_df = head_of_eval_df.style.set_properties(**{'text-align': 'left'})\n",
    "style_df = style_df.set_properties(**{\"vertical-align\": \"text-top\"})\n",
    "style_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {
    "id": "4lwmquXt8UcK"
   },
   "outputs": [],
   "source": [
    "# Save the Dataframe file\n",
    "eval_df.to_csv(\"eval_df.csv\")"
   ]
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "gpuType": "T4",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "venv_nlp",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
