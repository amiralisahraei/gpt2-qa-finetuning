{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ME_D6xegjD81"
      },
      "outputs": [],
      "source": [
        "!pip install jsonlines python-dotenv lamini datasets langchain_groq"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# to avoid conflit\n",
        "!pip install numpy==1.26.4\n",
        "!pip install --upgrade transformers"
      ],
      "metadata": {
        "id": "tnxojIy1rD8V"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": 99,
      "metadata": {
        "id": "qQog7K5zie8R"
      },
      "outputs": [],
      "source": [
        "import os\n",
        "import numpy as np\n",
        "import pickle\n",
        "import re\n",
        "import jsonlines\n",
        "import pandas as pd\n",
        "from pprint import pprint\n",
        "from dotenv import load_dotenv\n",
        "import lamini\n",
        "import logging\n",
        "from datasets import load_dataset, Dataset\n",
        "from transformers import (\n",
        "    AutoTokenizer,\n",
        "    AutoModelForCausalLM,\n",
        "    DataCollatorWithPadding,\n",
        "    AutoModelForMaskedLM,\n",
        "    TrainingArguments,\n",
        "    Trainer\n",
        ")\n",
        "import torch\n",
        "from peft import PeftModel, PeftConfig, get_peft_model, LoraConfig\n",
        "from langchain_groq import ChatGroq\n",
        "from langchain_core.prompts import ChatPromptTemplate\n",
        "from langchain.evaluation.qa import QAEvalChain"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {
        "id": "zqsMbT9Fie8c"
      },
      "outputs": [],
      "source": [
        "os.environ['GROQ_API_KEY'] = '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'"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "6hO-2CEcie9B"
      },
      "source": [
        "## 🔴 Load dataset and tokenization"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### 🟡 Load Dataset from HugginFace"
      ],
      "metadata": {
        "id": "sibZE7B9RVZo"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "7F_QMDbwie9C"
      },
      "outputs": [],
      "source": [
        "finetuning_dataset = load_dataset(\"AmiraliSH/lamini\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 21,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "FawuveRzie9E",
        "outputId": "2060f2a0-7693-4ac0-9232-e11dd1873804"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Dataset({\n",
            "    features: ['question', 'answer'],\n",
            "    num_rows: 1120\n",
            "})\n",
            "Dataset({\n",
            "    features: ['question', 'answer'],\n",
            "    num_rows: 280\n",
            "})\n"
          ]
        }
      ],
      "source": [
        "train_dataset, test_dataset = finetuning_dataset[\"train\"], finetuning_dataset[\"test\"]\n",
        "print(train_dataset)\n",
        "print(test_dataset)"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### 🟡 Load Base Model"
      ],
      "metadata": {
        "id": "Zw_1V8aARij6"
      }
    },
    {
      "cell_type": "code",
      "execution_count": 6,
      "metadata": {
        "id": "VzHfeiF6ie9E"
      },
      "outputs": [],
      "source": [
        "model_name = \"openai-community/gpt2\"\n",
        "tokenizer = AutoTokenizer.from_pretrained(model_name)\n",
        "model = AutoModelForCausalLM.from_pretrained(model_name)"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### 🟡 Add padding token\n",
        "<p>Use EOS token as padding if no pad token is set, ensuring compatibility during tokenization.</p>"
      ],
      "metadata": {
        "id": "ppRnBwu0RtPb"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### 🟡 Add inference to simplify the prediction of the model"
      ],
      "metadata": {
        "id": "LUEQDXSqSAyW"
      }
    },
    {
      "cell_type": "code",
      "execution_count": 7,
      "metadata": {
        "id": "EzFVKi6rOo2V"
      },
      "outputs": [],
      "source": [
        "if tokenizer.pad_token is None:\n",
        "    tokenizer.pad_token = tokenizer.eos_token"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 78,
      "metadata": {
        "id": "RSOqLGSIie9F"
      },
      "outputs": [],
      "source": [
        "def inference(text, model, tokenizer, max_output_tokens=100):\n",
        "    \"\"\"\n",
        "    Generates a model-based response for a given input text using a tokenizer and a language model.\n",
        "\n",
        "    Parameters\n",
        "    ----------\n",
        "    text : str\n",
        "        The input text to be processed and used as a prompt for the model.\n",
        "    model : transformers.PreTrainedModel\n",
        "        The pre-trained model used for generating text.\n",
        "    tokenizer : transformers.PreTrainedTokenizer\n",
        "        The tokenizer associated with the model, used for tokenizing the input text and decoding the output tokens.\n",
        "    max_output_tokens : int, optional\n",
        "        The maximum number of tokens for the output sequence. Default is 100.\n",
        "\n",
        "    Returns\n",
        "    -------\n",
        "    str\n",
        "        The generated response text after removing the input prompt from the output.\n",
        "    \"\"\"\n",
        "\n",
        "    # Tokenize\n",
        "    inputs = tokenizer(\n",
        "            text,\n",
        "            return_tensors=\"pt\",\n",
        "            truncation=True,\n",
        "            max_length=1024\n",
        "    )\n",
        "\n",
        "    input_ids = inputs[\"input_ids\"]\n",
        "    attention_mask = inputs[\"attention_mask\"]\n",
        "\n",
        "    # Generate\n",
        "    device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "    model.to(device)\n",
        "    generated_tokens_with_prompt = model.generate(\n",
        "      input_ids=input_ids.to(device),\n",
        "      attention_mask=attention_mask.to(device),\n",
        "      max_new_tokens=max_output_tokens,\n",
        "      pad_token_id=tokenizer.eos_token_id\n",
        "    )\n",
        "\n",
        "    # Decode\n",
        "    generated_text_with_prompt = tokenizer.batch_decode(generated_tokens_with_prompt, skip_special_tokens=True)\n",
        "\n",
        "    # Strip the prompt\n",
        "    generated_text_answer = generated_text_with_prompt[0][len(text):]\n",
        "\n",
        "    return generated_text_answer"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 79,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "I_XElYiyie9G",
        "outputId": "0fab672c-b7ac-4ea9-edd4-c0cac5d2f557"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "{'answer': 'Lamini can be used for any type of content generation, including '\n",
            "           'creative writing. Try adapting one of our examples or walkthroughs '\n",
            "           'to your use case. You can find these examples in our '\n",
            "           'documentation.',\n",
            " 'question': '### Question:\\n'\n",
            "             'Are there any tutorials on using Lamini for content generation '\n",
            "             'in creative writing?\\n'\n",
            "             '\\n'\n",
            "             '### Answer:'}\n",
            "('\\n'\n",
            " '\\n'\n",
            " 'Lamini is a free, open source, open source, open source, open source, open '\n",
            " 'source, open source, open source, open source, open source, open source, '\n",
            " 'open source, open source, open source, open source, open source, open '\n",
            " 'source, open source, open source, open source, open source, open source, '\n",
            " 'open source, open source, open source, open source, open source, open '\n",
            " 'source, open source, open source, open source, open')\n"
          ]
        }
      ],
      "source": [
        "# Test the Interface for one sample\n",
        "test_sample = test_dataset[0]\n",
        "pprint(test_sample)\n",
        "pprint(inference(test_sample[\"question\"], model, tokenizer))"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### 🟡 Tokenizer"
      ],
      "metadata": {
        "id": "9VX0n2nsS2pZ"
      }
    },
    {
      "cell_type": "code",
      "execution_count": 10,
      "metadata": {
        "id": "Vgs2xCOoie9G"
      },
      "outputs": [],
      "source": [
        "def tokenize_function(examples):\n",
        "    \"\"\"\n",
        "    Tokenizes question-answer pairs from the dataset with truncation.\n",
        "\n",
        "    Parameters\n",
        "    ----------\n",
        "    examples : dict\n",
        "        A batch of examples containing \"question\" and \"answer\" fields.\n",
        "\n",
        "    Returns\n",
        "    -------\n",
        "    dict\n",
        "        Tokenized inputs with the specified truncation and maximum length.\n",
        "    \"\"\"\n",
        "\n",
        "    text = [q+a for q, a in zip(examples[\"question\"], examples[\"answer\"])]\n",
        "\n",
        "    tokenizer.truncation_side = \"left\"\n",
        "    tokenized_output = tokenizer(\n",
        "        text,\n",
        "        truncation=True,\n",
        "        padding=\"max_length\",  # Ensures all sequences have the same length\n",
        "        max_length=1024\n",
        "    )\n",
        "\n",
        "    return tokenized_output"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 23,
      "metadata": {
        "id": "qdKpLtXtie9G",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 49,
          "referenced_widgets": [
            "063a90eb4a144b2e9874fc01c0227ad4",
            "7a12c82dea9b4a8db47dd990de038430",
            "cba9b916b9aa4a27a84d3de14858c1cc",
            "c4c0cb06a38146818efe42f2461e8edc",
            "d878737153224853a52d9632a7897773",
            "dda7d4b00f1443c9841bb6434f5a2d25",
            "5bdd56b29b044927bda1427df8a24582",
            "ceddd32ae8974eb8bb4843e52f21a369",
            "29645754e81e422f80ecddee053891c7",
            "78fc88e5646644c0920f3504e78bbb21",
            "015622517e8a48cf98fade3831061203"
          ]
        },
        "outputId": "da9874ab-0a3c-4fd3-dcf7-01275744890f"
      },
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "Map:   0%|          | 0/1120 [00:00<?, ? examples/s]"
            ],
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "063a90eb4a144b2e9874fc01c0227ad4"
            }
          },
          "metadata": {}
        }
      ],
      "source": [
        "# Apply tokenizer on Train and Test datasets\n",
        "tokenized_train_dataset = train_dataset.map(\n",
        "    tokenize_function,\n",
        "    batched=True,\n",
        "    batch_size=32,\n",
        "    drop_last_batch=True\n",
        ")\n",
        "\n",
        "tokenized_test_dataset = test_dataset.map(\n",
        "    tokenize_function,\n",
        "    batched=True,\n",
        "    batch_size=32,\n",
        "    drop_last_batch=True\n",
        ")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 24,
      "metadata": {
        "id": "8A8-_02Yie9J"
      },
      "outputs": [],
      "source": [
        "# While input_ids provides the input tokens, labels explicitly specifies the targets the model should learn to predict during training\n",
        "tokenized_train_dataset = tokenized_train_dataset.add_column(\"labels\", tokenized_train_dataset[\"input_ids\"])\n",
        "tokenized_test_dataset = tokenized_test_dataset.add_column(\"labels\", tokenized_test_dataset[\"input_ids\"])"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# evaluation data\n",
        "eval_data_size = int(len(tokenized_train_dataset) * 0.2)\n",
        "tokenized_eval_dataset = tokenized_train_dataset.select(range(eval_data_size))\n",
        "final_train_dataset = tokenized_train_dataset.select(range(eval_data_size, len(tokenized_train_dataset)))"
      ],
      "metadata": {
        "id": "Qv8d6OWr4a32"
      },
      "execution_count": 25,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# create data collator\n",
        "data_collator = DataCollatorWithPadding(tokenizer=tokenizer)"
      ],
      "metadata": {
        "id": "XP6m2oGYIR-z"
      },
      "execution_count": 26,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "mcb_575rie9J"
      },
      "source": [
        "## 🔴 Training all the parameters"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### 🟡 Set the device"
      ],
      "metadata": {
        "id": "P10sLFcvTxWj"
      }
    },
    {
      "cell_type": "code",
      "execution_count": 27,
      "metadata": {
        "id": "mzhVqaAzie9K",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "b9657cf7-a5d3-4cf1-d722-8adbe851bc49"
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "GPT2LMHeadModel(\n",
              "  (transformer): GPT2Model(\n",
              "    (wte): Embedding(50257, 768)\n",
              "    (wpe): Embedding(1024, 768)\n",
              "    (drop): Dropout(p=0.1, inplace=False)\n",
              "    (h): ModuleList(\n",
              "      (0-11): 12 x GPT2Block(\n",
              "        (ln_1): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
              "        (attn): GPT2Attention(\n",
              "          (c_attn): Conv1D(nf=2304, nx=768)\n",
              "          (c_proj): Conv1D(nf=768, nx=768)\n",
              "          (attn_dropout): Dropout(p=0.1, inplace=False)\n",
              "          (resid_dropout): Dropout(p=0.1, inplace=False)\n",
              "        )\n",
              "        (ln_2): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
              "        (mlp): GPT2MLP(\n",
              "          (c_fc): Conv1D(nf=3072, nx=768)\n",
              "          (c_proj): Conv1D(nf=768, nx=3072)\n",
              "          (act): NewGELUActivation()\n",
              "          (dropout): Dropout(p=0.1, inplace=False)\n",
              "        )\n",
              "      )\n",
              "    )\n",
              "    (ln_f): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
              "  )\n",
              "  (lm_head): Linear(in_features=768, out_features=50257, bias=False)\n",
              ")"
            ]
          },
          "metadata": {},
          "execution_count": 27
        }
      ],
      "source": [
        "base_model = AutoModelForCausalLM.from_pretrained(model_name)\n",
        "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "base_model.to(device)\n",
        "base_model"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### 🟡 Train arguments"
      ],
      "metadata": {
        "id": "yCq0tfTkT_ZH"
      }
    },
    {
      "cell_type": "code",
      "execution_count": 32,
      "metadata": {
        "id": "phEGrwY9ie9N"
      },
      "outputs": [],
      "source": [
        "# Make a directory to save the model\n",
        "trained_model_name = f\"new_save_model_dir\"\n",
        "output_dir = trained_model_name\n",
        "\n",
        "training_args = TrainingArguments(\n",
        "\n",
        "  # Learning rate\n",
        "  learning_rate=1.0e-5,\n",
        "\n",
        "  # Number of training epochs\n",
        "  # num_train_epochs=5,\n",
        "\n",
        "  # Max steps to train for (each step is a batch of data)\n",
        "  # Overrides num_train_epochs, if not -1\n",
        "  max_steps=100,\n",
        "\n",
        "  # Batch size for training\n",
        "  per_device_train_batch_size=1,\n",
        "\n",
        "  # Directory to save model checkpoints\n",
        "  output_dir=output_dir,\n",
        "\n",
        "  # Other arguments\n",
        "  overwrite_output_dir=False, # Overwrite the content of the output directory\n",
        "  disable_tqdm=False, # Disable progress bars\n",
        "  eval_steps=5, # Number of update steps between two evaluations\n",
        "  save_steps=5, # After # steps model is saved\n",
        "  warmup_steps=1, # Number of warmup steps for learning rate scheduler\n",
        "  per_device_eval_batch_size=1, # Batch size for evaluation\n",
        "  eval_strategy=\"steps\",\n",
        "  logging_strategy=\"steps\",\n",
        "  logging_steps=1,\n",
        "  optim=\"adafactor\",\n",
        "  gradient_accumulation_steps = 4,\n",
        "  gradient_checkpointing=False,\n",
        "\n",
        "  # Parameters for early stopping\n",
        "  load_best_model_at_end=True,\n",
        "  save_total_limit=1,\n",
        "  metric_for_best_model=\"eval_loss\",\n",
        "  greater_is_better=False,\n",
        "\n",
        "  save_safetensors=False\n",
        ")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### 🟡 Calculate Memory footprint and Flops on input shape and gradient accumulation steps"
      ],
      "metadata": {
        "id": "W-mYnuR0UFVT"
      }
    },
    {
      "cell_type": "code",
      "execution_count": 33,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "T0AmwJtDie9N",
        "outputId": "14d17482-bb94-48c2-8f32-dc9b2958b167"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "GPT2LMHeadModel(\n",
            "  (transformer): GPT2Model(\n",
            "    (wte): Embedding(50257, 768)\n",
            "    (wpe): Embedding(1024, 768)\n",
            "    (drop): Dropout(p=0.1, inplace=False)\n",
            "    (h): ModuleList(\n",
            "      (0-11): 12 x GPT2Block(\n",
            "        (ln_1): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
            "        (attn): GPT2Attention(\n",
            "          (c_attn): Conv1D(nf=2304, nx=768)\n",
            "          (c_proj): Conv1D(nf=768, nx=768)\n",
            "          (attn_dropout): Dropout(p=0.1, inplace=False)\n",
            "          (resid_dropout): Dropout(p=0.1, inplace=False)\n",
            "        )\n",
            "        (ln_2): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
            "        (mlp): GPT2MLP(\n",
            "          (c_fc): Conv1D(nf=3072, nx=768)\n",
            "          (c_proj): Conv1D(nf=768, nx=3072)\n",
            "          (act): NewGELUActivation()\n",
            "          (dropout): Dropout(p=0.1, inplace=False)\n",
            "        )\n",
            "      )\n",
            "    )\n",
            "    (ln_f): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
            "  )\n",
            "  (lm_head): Linear(in_features=768, out_features=50257, bias=False)\n",
            ")\n",
            "Memory footprint 0.510342192 GB\n",
            "Flops 2090.336256 GFLOPs\n"
          ]
        }
      ],
      "source": [
        "model_flops = (\n",
        "  base_model.floating_point_ops(\n",
        "    {\n",
        "       \"input_ids\": torch.zeros(\n",
        "           (1, 1024)\n",
        "      )\n",
        "    }\n",
        "  )\n",
        "  * training_args.gradient_accumulation_steps\n",
        ")\n",
        "\n",
        "print(\"Memory footprint\", base_model.get_memory_footprint() / 1e9, \"GB\")\n",
        "print(\"Flops\", model_flops / 1e9, \"GFLOPs\")"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### 🟡 Train the model"
      ],
      "metadata": {
        "id": "DKJoIDc5UfM5"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def compute_metrics(eval_pred):\n",
        "    logits, labels = eval_pred\n",
        "    predictions = np.argmax(logits, axis=-1)\n",
        "    return {\"accuracy\": (predictions == labels).mean()}"
      ],
      "metadata": {
        "id": "AlXnd-8CcvqD"
      },
      "execution_count": 34,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": 35,
      "metadata": {
        "id": "p2-tNZAvie9P"
      },
      "outputs": [],
      "source": [
        "\"\"\"\n",
        "Initialize the Trainer to manage the entire training process, including model training, evaluation,\n",
        "and logging. It uses the provided model, training arguments, and tokenized datasets for training\n",
        "and evaluation, handling tasks like optimization, checkpointing, and metric calculation.\n",
        "\"\"\"\n",
        "trainer = Trainer(\n",
        "    model=base_model,\n",
        "    args=training_args,\n",
        "    train_dataset=final_train_dataset,\n",
        "    eval_dataset=tokenized_eval_dataset,\n",
        "    data_collator=data_collator,\n",
        "    # compute_metrics=compute_metrics\n",
        ")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 36,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 882
        },
        "id": "ioHAixoKie9P",
        "outputId": "08ea9acc-11ff-428e-e463-f62ec9830e72"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m The `run_name` is currently set to the same value as `TrainingArguments.output_dir`. If this was not intended, please specify a different run name by setting the `TrainingArguments.run_name` parameter.\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: Using wandb-core as the SDK backend.  Please refer to https://wandb.me/wandb-core for more information.\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: Currently logged in as: \u001b[33ma-sahraei98\u001b[0m (\u001b[33ma-sahraei98-\u001b[0m) to \u001b[32mhttps://api.wandb.ai\u001b[0m. Use \u001b[1m`wandb login --relogin`\u001b[0m to force relogin\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "Tracking run with wandb version 0.19.8"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "Run data is saved locally in <code>/content/wandb/run-20250321_181104-1wqhcz90</code>"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "Syncing run <strong><a href='https://wandb.ai/a-sahraei98-/huggingface/runs/1wqhcz90' target=\"_blank\">new_save_model_dir</a></strong> to <a href='https://wandb.ai/a-sahraei98-/huggingface' target=\"_blank\">Weights & Biases</a> (<a href='https://wandb.me/developer-guide' target=\"_blank\">docs</a>)<br>"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              " View project at <a href='https://wandb.ai/a-sahraei98-/huggingface' target=\"_blank\">https://wandb.ai/a-sahraei98-/huggingface</a>"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              " View run at <a href='https://wandb.ai/a-sahraei98-/huggingface/runs/1wqhcz90' target=\"_blank\">https://wandb.ai/a-sahraei98-/huggingface/runs/1wqhcz90</a>"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "`loss_type=None` was set in the config but it is unrecognised.Using the default loss: `ForCausalLMLoss`.\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "\n",
              "    <div>\n",
              "      \n",
              "      <progress value='100' max='100' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
              "      [100/100 10:03, Epoch 0/1]\n",
              "    </div>\n",
              "    <table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              " <tr style=\"text-align: left;\">\n",
              "      <th>Step</th>\n",
              "      <th>Training Loss</th>\n",
              "      <th>Validation Loss</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <td>5</td>\n",
              "      <td>6.007200</td>\n",
              "      <td>6.801677</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>10</td>\n",
              "      <td>1.798800</td>\n",
              "      <td>1.628249</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>15</td>\n",
              "      <td>0.458800</td>\n",
              "      <td>0.315967</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>20</td>\n",
              "      <td>0.383600</td>\n",
              "      <td>0.273885</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>25</td>\n",
              "      <td>0.300100</td>\n",
              "      <td>0.259380</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>30</td>\n",
              "      <td>0.232100</td>\n",
              "      <td>0.246287</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>35</td>\n",
              "      <td>0.280700</td>\n",
              "      <td>0.236290</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>40</td>\n",
              "      <td>0.208800</td>\n",
              "      <td>0.230925</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>45</td>\n",
              "      <td>0.190200</td>\n",
              "      <td>0.225085</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>50</td>\n",
              "      <td>0.258200</td>\n",
              "      <td>0.220617</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>55</td>\n",
              "      <td>0.176900</td>\n",
              "      <td>0.217713</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>60</td>\n",
              "      <td>0.222900</td>\n",
              "      <td>0.214924</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>65</td>\n",
              "      <td>0.222000</td>\n",
              "      <td>0.212555</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>70</td>\n",
              "      <td>0.246700</td>\n",
              "      <td>0.210723</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>75</td>\n",
              "      <td>0.249500</td>\n",
              "      <td>0.209427</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>80</td>\n",
              "      <td>0.190200</td>\n",
              "      <td>0.208316</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>85</td>\n",
              "      <td>0.207000</td>\n",
              "      <td>0.207390</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>90</td>\n",
              "      <td>0.238600</td>\n",
              "      <td>0.206877</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>95</td>\n",
              "      <td>0.234000</td>\n",
              "      <td>0.206528</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>100</td>\n",
              "      <td>0.133300</td>\n",
              "      <td>0.206392</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table><p>"
            ]
          },
          "metadata": {}
        }
      ],
      "source": [
        "training_output = trainer.train()"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### 🟡 Save the model"
      ],
      "metadata": {
        "id": "-hV17cGiUlMo"
      }
    },
    {
      "cell_type": "code",
      "execution_count": 37,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "UqDSRiMUie9Q",
        "outputId": "1c7401ff-9dc1-4376-b5c6-e584c06152c4"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Saved model to: new_save_model_dir/final\n"
          ]
        }
      ],
      "source": [
        "# Save the model\n",
        "save_dir = f'{output_dir}/final'\n",
        "\n",
        "trainer.save_model(save_dir)\n",
        "print(\"Saved model to:\", save_dir)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "7C2m46SHie9Q"
      },
      "source": [
        "## 🔴 Evaluation the performance of model with help of another LLM"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### 🟡 Transfer the model to evaluation mode"
      ],
      "metadata": {
        "id": "7qtEZ079VEnd"
      }
    },
    {
      "cell_type": "code",
      "execution_count": 38,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "dK37X_ygie9Q",
        "outputId": "33f7de51-6424-4df6-cb09-316dfbe457d4"
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "GPT2LMHeadModel(\n",
              "  (transformer): GPT2Model(\n",
              "    (wte): Embedding(50257, 768)\n",
              "    (wpe): Embedding(1024, 768)\n",
              "    (drop): Dropout(p=0.1, inplace=False)\n",
              "    (h): ModuleList(\n",
              "      (0-11): 12 x GPT2Block(\n",
              "        (ln_1): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
              "        (attn): GPT2Attention(\n",
              "          (c_attn): Conv1D(nf=2304, nx=768)\n",
              "          (c_proj): Conv1D(nf=768, nx=768)\n",
              "          (attn_dropout): Dropout(p=0.1, inplace=False)\n",
              "          (resid_dropout): Dropout(p=0.1, inplace=False)\n",
              "        )\n",
              "        (ln_2): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
              "        (mlp): GPT2MLP(\n",
              "          (c_fc): Conv1D(nf=3072, nx=768)\n",
              "          (c_proj): Conv1D(nf=768, nx=3072)\n",
              "          (act): NewGELUActivation()\n",
              "          (dropout): Dropout(p=0.1, inplace=False)\n",
              "        )\n",
              "      )\n",
              "    )\n",
              "    (ln_f): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
              "  )\n",
              "  (lm_head): Linear(in_features=768, out_features=50257, bias=False)\n",
              ")"
            ]
          },
          "metadata": {},
          "execution_count": 38
        }
      ],
      "source": [
        "finetuned_slightly_model = AutoModelForCausalLM.from_pretrained(save_dir, local_files_only=True)\n",
        "finetuned_slightly_model.to(device)\n",
        "finetuned_slightly_model.eval()"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### 🟡 Prepare test data for evaluation and prediction for eval data\n"
      ],
      "metadata": {
        "id": "7Dyy3Yo-VodE"
      }
    },
    {
      "cell_type": "code",
      "execution_count": 40,
      "metadata": {
        "id": "UO5ws0wrie9R"
      },
      "outputs": [],
      "source": [
        "q_a_test_dataset = [{\"query\": q, \"answer\": a} for q, a in zip(test_dataset[\"question\"], test_dataset[\"answer\"])]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 41,
      "metadata": {
        "id": "2vP46JcKie9S"
      },
      "outputs": [],
      "source": [
        "prediction = []\n",
        "for sample in q_a_test_dataset:\n",
        "    question = sample[\"query\"]\n",
        "    answer = sample[\"answer\"]\n",
        "    finetuned_model_answer = inference(question, finetuned_slightly_model, tokenizer)\n",
        "    output = {\"query\": question, \"answer\": answer, \"result\": finetuned_model_answer}\n",
        "    prediction.append(output)"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### 🟡 Initialize another LLM for evaluation"
      ],
      "metadata": {
        "id": "CgenoLdeWT9k"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def initialize_llm(model_name):\n",
        "    return ChatGroq(\n",
        "        model=model_name,\n",
        "        temperature=0,\n",
        "        max_tokens=None,\n",
        "        timeout=None,\n",
        "        streaming=True,\n",
        "    )"
      ],
      "metadata": {
        "id": "BwgosTeA-NwT"
      },
      "execution_count": 53,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": 61,
      "metadata": {
        "id": "warvYojLie9T"
      },
      "outputs": [],
      "source": [
        "MODEL_NAME = \"deepseek-r1-distill-llama-70b\"\n",
        "llm = initialize_llm(MODEL_NAME)\n",
        "eval_chain = QAEvalChain.from_llm(llm)\n",
        "result = eval_chain.evaluate(q_a_test_dataset, prediction)"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### 🟡 Count Accuracy"
      ],
      "metadata": {
        "id": "QYSSdRyhW3xs"
      }
    },
    {
      "cell_type": "code",
      "execution_count": 71,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "-Pn6X0ONie9U",
        "outputId": "5d691283-e835-4871-9cfe-ed22b549e802"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Accuracy: 49.64%\n"
          ]
        }
      ],
      "source": [
        "eval_result = [1 if i[\"results\"].split(\"\\n\")[-1] == \"GRADE: CORRECT\" else 0 for i in result]\n",
        "eval_result = (sum(eval_result) / len(eval_result)) * 100\n",
        "print(f\"Accuracy: {eval_result:.2f}%\")"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### 🟡 Save the result of evaluation in a Datafraem"
      ],
      "metadata": {
        "id": "OLJ8N6kaXFXJ"
      }
    },
    {
      "cell_type": "code",
      "execution_count": 72,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 554
        },
        "id": "LSyvfypp7zZT",
        "outputId": "df2c1d0d-c8d3-495e-d454-2cf1adb39b50"
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<pandas.io.formats.style.Styler at 0x7c398ca16d90>"
            ],
            "text/html": [
              "<style type=\"text/css\">\n",
              "#T_228ed_row0_col0, #T_228ed_row0_col1, #T_228ed_row0_col2, #T_228ed_row1_col0, #T_228ed_row1_col1, #T_228ed_row1_col2, #T_228ed_row2_col0, #T_228ed_row2_col1, #T_228ed_row2_col2, #T_228ed_row3_col0, #T_228ed_row3_col1, #T_228ed_row3_col2, #T_228ed_row4_col0, #T_228ed_row4_col1, #T_228ed_row4_col2 {\n",
              "  text-align: left;\n",
              "  vertical-align: text-top;\n",
              "}\n",
              "</style>\n",
              "<table id=\"T_228ed\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr>\n",
              "      <th class=\"blank level0\" >&nbsp;</th>\n",
              "      <th id=\"T_228ed_level0_col0\" class=\"col_heading level0 col0\" >query</th>\n",
              "      <th id=\"T_228ed_level0_col1\" class=\"col_heading level0 col1\" >answer</th>\n",
              "      <th id=\"T_228ed_level0_col2\" class=\"col_heading level0 col2\" >result</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th id=\"T_228ed_level0_row0\" class=\"row_heading level0 row0\" >0</th>\n",
              "      <td id=\"T_228ed_row0_col0\" class=\"data row0 col0\" >### Question:\n",
              "Are there any tutorials on using Lamini for content generation in creative writing?\n",
              "\n",
              "### Answer:</td>\n",
              "      <td id=\"T_228ed_row0_col1\" class=\"data row0 col1\" >Lamini can be used for any type of content generation, including creative writing. Try adapting one of our examples or walkthroughs to your use case. You can find these examples in our documentation.</td>\n",
              "      <td id=\"T_228ed_row0_col2\" class=\"data row0 col2\" >Yes, there are tutorials on using Lamini for content generation in creative writing.</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th id=\"T_228ed_level0_row1\" class=\"row_heading level0 row1\" >1</th>\n",
              "      <td id=\"T_228ed_row1_col0\" class=\"data row1 col0\" >### Question:\n",
              "Can Lamini be used to perform sentiment analysis or opinion mining on large volumes of text data?\n",
              "\n",
              "### Answer:</td>\n",
              "      <td id=\"T_228ed_row1_col1\" class=\"data row1 col1\" >Lamini can be used for sentiment analysis or opinion mining on large volumes of text data. To learn how, check out walkthroughs and examples available on Lamini’s website. With some imagination, you can adapt those examples to your data and use case.</td>\n",
              "      <td id=\"T_228ed_row1_col2\" class=\"data row1 col2\" >Yes, Lamini can be used to perform sentiment analysis or opinion mining on large volumes of text data. Lamini can be used to perform sentiment analysis or opinion mining on large volumes of text data.</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th id=\"T_228ed_level0_row2\" class=\"row_heading level0 row2\" >2</th>\n",
              "      <td id=\"T_228ed_row2_col0\" class=\"data row2 col0\" >### Question:\n",
              "Do I have to pay for using Lamini?\n",
              "\n",
              "### Answer:</td>\n",
              "      <td id=\"T_228ed_row2_col1\" class=\"data row2 col1\" >Everyone starts with 10,000 free credits, which is equivalent to about $100. After that, you can purchase more credits in the “API” tab at app.lamini.ai.</td>\n",
              "      <td id=\"T_228ed_row2_col2\" class=\"data row2 col2\" >Yes, Lamini is free to use. However, you may need to pay for the Lamini software to use it.</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th id=\"T_228ed_level0_row3\" class=\"row_heading level0 row3\" >3</th>\n",
              "      <td id=\"T_228ed_row3_col0\" class=\"data row3 col0\" >### Question:\n",
              "Can Lamini understand and generate text in multiple languages?\n",
              "\n",
              "### Answer:</td>\n",
              "      <td id=\"T_228ed_row3_col1\" class=\"data row3 col1\" >Yes, Lamini can understand and generate text in multiple languages. It currently supports over 20 languages, including English, Spanish, French, German, Chinese, and Japanese.</td>\n",
              "      <td id=\"T_228ed_row3_col2\" class=\"data row3 col2\" >Yes, Lamini can generate text in multiple languages. Lamini can generate text in multiple languages.</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th id=\"T_228ed_level0_row4\" class=\"row_heading level0 row4\" >4</th>\n",
              "      <td id=\"T_228ed_row4_col0\" class=\"data row4 col0\" >### Question:\n",
              "Can Lamini talk to animals or understand what they're saying?\n",
              "\n",
              "### Answer:</td>\n",
              "      <td id=\"T_228ed_row4_col1\" class=\"data row4 col1\" >While Lamini possesses extraordinary linguistic capabilities, it is crucial to note that its abilities do not extend to conversing with our animal counterparts or comprehending their communications. As an AI language model, Lamini's domain of expertise revolves around processing and generating text, responding to human inquiries and prompts with remarkable precision. While the enigmatic language of animals remains beyond its purview, Lamini's prowess in linguistic understanding and contextual interpretation continues to astound, forging new frontiers in human-machine interactions. While our fascination with bridging the gap between human and animal communication endures, Lamini's current capacities remain focused on enhancing our understanding of language and facilitating meaningful dialogue in the realms of human discourse.</td>\n",
              "      <td id=\"T_228ed_row4_col2\" class=\"data row4 col2\" >Yes, Lamini can talk to animals or understand what they're saying. Lamini can talk to animals or understand what they're saying.</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n"
            ]
          },
          "metadata": {},
          "execution_count": 72
        }
      ],
      "source": [
        "eval_df = pd.DataFrame.from_dict(prediction)\n",
        "head_of_eval_df = eval_df.head()\n",
        "style_df = head_of_eval_df.style.set_properties(**{'text-align': 'left'})\n",
        "style_df = style_df.set_properties(**{\"vertical-align\": \"text-top\"})\n",
        "style_df"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "4lwmquXt8UcK"
      },
      "outputs": [],
      "source": [
        "# Save the Dataframe file\n",
        "eval_df.to_csv(\"eval_df.csv\")"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# 🔴 LORA"
      ],
      "metadata": {
        "id": "oalzugTONFYE"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### 🟡 Config LORA"
      ],
      "metadata": {
        "id": "nuCa4HRce_Xh"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "peft_config = LoraConfig(\n",
        "    task_type=\"CAUSAL_LM\",  # Task type for GPT-2\n",
        "    r=4,  # Rank of the low-rank matrices\n",
        "    lora_alpha=32,  # Scaling factor for LoRA weights\n",
        "    lora_dropout=0.01,  # Dropout for LoRA layers\n",
        "    target_modules=[\"c_attn\"],  # Target the combined attention layer\n",
        "    fan_in_fan_out=True,  # Set this to True for Conv1D layers\n",
        ")"
      ],
      "metadata": {
        "id": "Zy-irESKNjMU"
      },
      "execution_count": 92,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "model = get_peft_model(base_model, peft_config)"
      ],
      "metadata": {
        "id": "BjbwrJucODs9"
      },
      "execution_count": 93,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Print trainable parameters\n",
        "model.print_trainable_parameters()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "JzJQIYP7OJsr",
        "outputId": "892c2b15-0f19-4994-9f3c-6ff2e9a5b1bc"
      },
      "execution_count": 94,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "trainable params: 147,456 || all params: 124,587,264 || trainable%: 0.1184\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### 🟡 Traning"
      ],
      "metadata": {
        "id": "FO7aCTk_fFgf"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "training_args = TrainingArguments(\n",
        "    # Learning rate\n",
        "    learning_rate=1.0e-5,\n",
        "\n",
        "    # Number of training epochs\n",
        "    num_train_epochs=5,\n",
        "\n",
        "    # Batch size for training\n",
        "    per_device_train_batch_size=8,\n",
        "\n",
        "    # Directory to save model checkpoints\n",
        "    output_dir=\"/content/LORA\",\n",
        "\n",
        "    # Other arguments\n",
        "    overwrite_output_dir=False,  # Overwrite the content of the output directory\n",
        "    disable_tqdm=False,  # Disable progress bars\n",
        "    warmup_steps=1,  # Number of warmup steps for learning rate scheduler\n",
        "    per_device_eval_batch_size=1,  # Batch size for evaluation\n",
        "    eval_strategy=\"epoch\",  # Evaluate at the end of each epoch\n",
        "    logging_strategy=\"epoch\",  # Log metrics at the end of each epoch\n",
        "    save_strategy=\"epoch\",  # Save model at the end of each epoch\n",
        "    optim=\"adafactor\",\n",
        "    gradient_accumulation_steps=4,\n",
        "    gradient_checkpointing=False,\n",
        "\n",
        "    # Parameters for early stopping\n",
        "    load_best_model_at_end=True,\n",
        "    save_total_limit=1,\n",
        "    metric_for_best_model=\"eval_loss\",\n",
        "    greater_is_better=False,\n",
        "\n",
        "    save_safetensors=False,\n",
        "    report_to=\"all\",  # Report metrics to all available logging integrations\n",
        ")"
      ],
      "metadata": {
        "id": "BqEqK_ayPoK5"
      },
      "execution_count": 97,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Initialize Trainer\n",
        "trainer = Trainer(\n",
        "    model=model,\n",
        "    args=training_args,\n",
        "    train_dataset=final_train_dataset,\n",
        "    eval_dataset=tokenized_eval_dataset,\n",
        "    # data_collator=data_collator,\n",
        ")\n",
        "\n",
        "# Train the model\n",
        "trainer.train()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 306
        },
        "id": "uKEtfNRwPPVr",
        "outputId": "d8dbde7b-9b20-4370-b7e2-a53b8fba912a"
      },
      "execution_count": 98,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "No label_names provided for model class `PeftModelForCausalLM`. Since `PeftModel` hides base models input arguments, if label_names is not given, label_names can't be set automatically within `Trainer`. Note that empty label_names list will be used instead.\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "\n",
              "    <div>\n",
              "      \n",
              "      <progress value='140' max='140' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
              "      [140/140 21:14, Epoch 5/5]\n",
              "    </div>\n",
              "    <table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              " <tr style=\"text-align: left;\">\n",
              "      <th>Epoch</th>\n",
              "      <th>Training Loss</th>\n",
              "      <th>Validation Loss</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <td>1</td>\n",
              "      <td>0.224300</td>\n",
              "      <td>0.206134</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>2</td>\n",
              "      <td>0.223700</td>\n",
              "      <td>0.205922</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>3</td>\n",
              "      <td>0.222800</td>\n",
              "      <td>0.205751</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>4</td>\n",
              "      <td>0.222600</td>\n",
              "      <td>0.205645</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>5</td>\n",
              "      <td>0.222900</td>\n",
              "      <td>0.205605</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table><p>"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "TrainOutput(global_step=140, training_loss=0.22327780042375836, metrics={'train_runtime': 1282.7673, 'train_samples_per_second': 3.492, 'train_steps_per_second': 0.109, 'total_flos': 2345235350814720.0, 'train_loss': 0.22327780042375836, 'epoch': 5.0})"
            ]
          },
          "metadata": {},
          "execution_count": 98
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Save the model\n",
        "save_dir = f'/content/LORA/final'\n",
        "\n",
        "trainer.save_model(save_dir)\n",
        "print(\"Saved model to:\", save_dir)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "cTvonWU7PwAl",
        "outputId": "052d848c-722a-49ec-fad9-0ce4688f1163"
      },
      "execution_count": 100,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Saved model to: /content/LORA/final\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### 🟡 Transfer the model to evaluation mode"
      ],
      "metadata": {
        "id": "VEAEuWw_ex7p"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "finetuned_slightly_model = AutoModelForCausalLM.from_pretrained(save_dir, local_files_only=True)\n",
        "finetuned_slightly_model.to(device)\n",
        "finetuned_slightly_model.eval()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "pMls68WUYQVL",
        "outputId": "6b4b6f9b-fc4a-4d12-cb99-07cce8a4a0e8"
      },
      "execution_count": 101,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "GPT2LMHeadModel(\n",
              "  (transformer): GPT2Model(\n",
              "    (wte): Embedding(50257, 768)\n",
              "    (wpe): Embedding(1024, 768)\n",
              "    (drop): Dropout(p=0.1, inplace=False)\n",
              "    (h): ModuleList(\n",
              "      (0-11): 12 x GPT2Block(\n",
              "        (ln_1): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
              "        (attn): GPT2Attention(\n",
              "          (c_attn): lora.Linear(\n",
              "            (base_layer): Conv1D(nf=2304, nx=768)\n",
              "            (lora_dropout): ModuleDict(\n",
              "              (default): Dropout(p=0.01, inplace=False)\n",
              "            )\n",
              "            (lora_A): ModuleDict(\n",
              "              (default): Linear(in_features=768, out_features=4, bias=False)\n",
              "            )\n",
              "            (lora_B): ModuleDict(\n",
              "              (default): Linear(in_features=4, out_features=2304, bias=False)\n",
              "            )\n",
              "            (lora_embedding_A): ParameterDict()\n",
              "            (lora_embedding_B): ParameterDict()\n",
              "            (lora_magnitude_vector): ModuleDict()\n",
              "          )\n",
              "          (c_proj): Conv1D(nf=768, nx=768)\n",
              "          (attn_dropout): Dropout(p=0.1, inplace=False)\n",
              "          (resid_dropout): Dropout(p=0.1, inplace=False)\n",
              "        )\n",
              "        (ln_2): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
              "        (mlp): GPT2MLP(\n",
              "          (c_fc): Conv1D(nf=3072, nx=768)\n",
              "          (c_proj): Conv1D(nf=768, nx=3072)\n",
              "          (act): NewGELUActivation()\n",
              "          (dropout): Dropout(p=0.1, inplace=False)\n",
              "        )\n",
              "      )\n",
              "    )\n",
              "    (ln_f): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
              "  )\n",
              "  (lm_head): Linear(in_features=768, out_features=50257, bias=False)\n",
              ")"
            ]
          },
          "metadata": {},
          "execution_count": 101
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### 🟡 Prediction for eval data\n"
      ],
      "metadata": {
        "id": "Z15HdP65YnNw"
      }
    },
    {
      "cell_type": "code",
      "execution_count": 102,
      "metadata": {
        "id": "MI0gRR1mYnN2"
      },
      "outputs": [],
      "source": [
        "prediction = []\n",
        "for sample in q_a_test_dataset:\n",
        "    question = sample[\"query\"]\n",
        "    answer = sample[\"answer\"]\n",
        "    finetuned_model_answer = inference(question, finetuned_slightly_model, tokenizer)\n",
        "    output = {\"query\": question, \"answer\": answer, \"result\": finetuned_model_answer}\n",
        "    prediction.append(output)"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### 🟡 Initialize another LLM for evaluation"
      ],
      "metadata": {
        "id": "O78Y4TgIZAuP"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def initialize_llm(model_name):\n",
        "    return ChatGroq(\n",
        "        model=model_name,\n",
        "        temperature=0,\n",
        "        max_tokens=None,\n",
        "        timeout=None,\n",
        "        streaming=True,\n",
        "    )"
      ],
      "metadata": {
        "id": "XyS-SkbwZAuU"
      },
      "execution_count": 103,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": 104,
      "metadata": {
        "id": "wmhbhbLIZAuX"
      },
      "outputs": [],
      "source": [
        "MODEL_NAME = \"deepseek-r1-distill-llama-70b\"\n",
        "llm = initialize_llm(MODEL_NAME)\n",
        "eval_chain = QAEvalChain.from_llm(llm)\n",
        "result = eval_chain.evaluate(q_a_test_dataset, prediction)"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### 🟡 Count Accuracy"
      ],
      "metadata": {
        "id": "thaFYfy1a5HI"
      }
    },
    {
      "cell_type": "code",
      "execution_count": 105,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "298bbe4d-1253-4fd5-ac97-8d9c2b4cf9a0",
        "id": "JGVcOz6pa5HK"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Accuracy: 32.14%\n"
          ]
        }
      ],
      "source": [
        "eval_result = [1 if i[\"results\"].split(\"\\n\")[-1] == \"GRADE: CORRECT\" else 0 for i in result]\n",
        "eval_result = (sum(eval_result) / len(eval_result)) * 100\n",
        "print(f\"Accuracy: {eval_result:.2f}%\")"
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "Db708ANhb7f2"
      },
      "execution_count": null,
      "outputs": []
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "gpuType": "T4",
      "provenance": [],
      "collapsed_sections": [
        "6hO-2CEcie9B",
        "mcb_575rie9J",
        "yCq0tfTkT_ZH",
        "W-mYnuR0UFVT",
        "DKJoIDc5UfM5",
        "7C2m46SHie9Q",
        "7qtEZ079VEnd",
        "CgenoLdeWT9k",
        "QYSSdRyhW3xs",
        "oalzugTONFYE",
        "nuCa4HRce_Xh",
        "FO7aCTk_fFgf",
        "VEAEuWw_ex7p",
        "Z15HdP65YnNw",
        "O78Y4TgIZAuP"
      ]
    },
    "kernelspec": {
      "display_name": "venv_nlp",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.12.3"
    },
    "widgets": {
      "application/vnd.jupyter.widget-state+json": {
        "063a90eb4a144b2e9874fc01c0227ad4": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HBoxModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HBoxModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HBoxView",
            "box_style": "",
            "children": [
              "IPY_MODEL_7a12c82dea9b4a8db47dd990de038430",
              "IPY_MODEL_cba9b916b9aa4a27a84d3de14858c1cc",
              "IPY_MODEL_c4c0cb06a38146818efe42f2461e8edc"
            ],
            "layout": "IPY_MODEL_d878737153224853a52d9632a7897773"
          }
        },
        "7a12c82dea9b4a8db47dd990de038430": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_dda7d4b00f1443c9841bb6434f5a2d25",
            "placeholder": "​",
            "style": "IPY_MODEL_5bdd56b29b044927bda1427df8a24582",
            "value": "Map: 100%"
          }
        },
        "cba9b916b9aa4a27a84d3de14858c1cc": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "FloatProgressModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "FloatProgressModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "ProgressView",
            "bar_style": "success",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_ceddd32ae8974eb8bb4843e52f21a369",
            "max": 1120,
            "min": 0,
            "orientation": "horizontal",
            "style": "IPY_MODEL_29645754e81e422f80ecddee053891c7",
            "value": 1120
          }
        },
        "c4c0cb06a38146818efe42f2461e8edc": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_78fc88e5646644c0920f3504e78bbb21",
            "placeholder": "​",
            "style": "IPY_MODEL_015622517e8a48cf98fade3831061203",
            "value": " 1120/1120 [00:01&lt;00:00, 952.60 examples/s]"
          }
        },
        "d878737153224853a52d9632a7897773": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "dda7d4b00f1443c9841bb6434f5a2d25": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "5bdd56b29b044927bda1427df8a24582": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "ceddd32ae8974eb8bb4843e52f21a369": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "29645754e81e422f80ecddee053891c7": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "ProgressStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "ProgressStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "bar_color": null,
            "description_width": ""
          }
        },
        "78fc88e5646644c0920f3504e78bbb21": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "015622517e8a48cf98fade3831061203": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        }
      }
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}